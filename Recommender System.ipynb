{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgaben die zu erledigen sind:\n",
    "- Product Shelf bereinigen --> schlecht gekaufte Produkte\n",
    "- Produkte empfehlen die noch nicht eingekauft wurden\n",
    "\n",
    "3 mögliche Systeme:\n",
    "- Simple Recommender --> Top Products\n",
    "    - Decide on the metric or score to rate products on\n",
    "    - Calcualte the score for every product\n",
    "    - Sort the products based on the score and output the top results\n",
    "    \n",
    "- Content Based Recommender --> Uses item meta data\n",
    "- Collaborative Recommender --> Products from other user that looks pretty similar\n",
    "\n",
    "\n",
    "Problems:\n",
    "Contrarian to rating data, purchase data has no upper band of the evaluated score\n",
    "There is no explicit rating data, so it has to be generated from implicit information available\n",
    "\n",
    "Ressources:\n",
    "- https://medium.com/datadriveninvestor/how-to-build-a-recommendation-system-for-purchase-data-step-by-step-d6d7a78800b6\n",
    "- https://www.datacamp.com/community/tutorials/recommender-systems-python?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=1t1&utm_creative=229765585186&utm_targetid=aud-299261629574:dsa-473406569915&utm_loc_interest_ms=&utm_loc_physical_ms=20143&gclid=Cj0KCQiA-bjyBRCcARIsAFboWg2VIh0dOVFJAckISmcG52iT4y1gr732tQkw-i9-Pj2VubR5WXq4IWcaArhsEALw_wcB\n",
    "\n",
    "\n",
    "Performance:\n",
    "This might be an issue.\n",
    "A possible solution is: https://towardsdatascience.com/how-to-analyse-100s-of-gbs-of-data-on-your-laptop-with-python-f83363dda94\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # used for histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv (\"Recommender4Retail.csv\", index_col=[0]) # low_memory option is depricated\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the attributes (According to https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b)\n",
    "\n",
    "- order_id: order identifier\n",
    "- user_id: user identifier\n",
    "- eval_set: which evaluation set this order belongs in (we only have Prior and Train) - no test?\n",
    "- order_number: the order sequence number for this user (1 = first, n = nth)\n",
    "- order_dow: the day of the week the order was placed on\n",
    "- order_hour_of_day: the hour of the day the order was placed on\n",
    "- days_since_prior_order: days since the last order, capped at 30 (with NaNs for order_number = 1)\n",
    "- product_id: product identifier\n",
    "- add_to_cart_order: order in which each product was added to cart\n",
    "- reordered: 1 if this product has been ordered by this user in the past, 0 otherwise\n",
    "- product_name: name of the product\n",
    "- aisle_id: aisle identifier\n",
    "- department_id: department identifier\n",
    "- department: the name of the department\n",
    "- ailse: the name of the aisle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the Data\n",
    "'''\n",
    "- What is eval_set, order_dow, reordered and can what can they be used for?\n",
    "- Can the amount a product has been bought, seen as a implicit ratings? There might be some problems to it depending on the product...\n",
    "- \n",
    "'''\n",
    "data.info()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique Values for specified colums\n",
    "cols = ('eval_set','order_dow','reordered')\n",
    "for var in cols:\n",
    "    res = data[var].unique()\n",
    "    print('{2}: UniqueValues {0}, No. of unique values {1} '.format(res,len(res),var))\n",
    "\n",
    "\n",
    "#Count occurence of each value\n",
    "#print(data.eval_set.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of departments\n",
    "n_of_departments = data.department.nunique()\n",
    "\n",
    "# amount of products bought by user\n",
    "# show the top 10 user with most products bought\n",
    "n_by_user = data.groupby('user_id')['product_id'].count()\n",
    "print(n_by_user.nlargest(10))\n",
    "\n",
    "# number of times a product is bought\n",
    "# show the products most bought\n",
    "n_of_products_bought = data.product_name.value_counts()\n",
    "n_of_products_bought_20 = n_of_products_bought.nlargest(20)\n",
    "\n",
    "plt.figure()\n",
    "sns.barplot(x=n_of_products_bought_20.index, y=n_of_products_bought_20.values)\n",
    "plt.xlabel('Products most ordered')\n",
    "plt.ylabel('Number of Orders')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig('prod_most_bought.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of products bought per order\n",
    "n_prod_order = data.groupby('order_id')['product_id'].count().value_counts().sort_index() # first row = number of products per order, second row = how many orders\n",
    "# This does not consider if a product is bought multiple times in one order, but i guess thats not in the dataset anyways?\n",
    "\n",
    "n_prod_order_n = n_prod_order.head(50) # limit to only plot the first n values\n",
    "\n",
    "plt.figure()\n",
    "sns.barplot(x=n_prod_order_n.index, y=n_prod_order_n.values)\n",
    "plt.xlabel('Number of Products per Order')\n",
    "plt.ylabel('Orders count')\n",
    "\n",
    "# Adjust the lenght of the plot. Source: https://stackoverflow.com/questions/44863375/how-to-change-spacing-between-ticks-in-matplotlib\n",
    "N = 50\n",
    "plt.gca().margins(x=0)\n",
    "plt.gcf().canvas.draw()\n",
    "tl = plt.gca().get_xticklabels()\n",
    "maxsize = max([t.get_window_extent().width for t in tl])\n",
    "m = 0.2 # inch margin\n",
    "s = maxsize/plt.gcf().dpi*N+2*m\n",
    "margin = m/plt.gcf().get_size_inches()[0]\n",
    "\n",
    "plt.gcf().subplots_adjust(left=margin, right=1.-margin)\n",
    "plt.gcf().set_size_inches(s, plt.gcf().get_size_inches()[1])\n",
    "\n",
    "plt.savefig('n_prod_order.png', bbox_inches='tight')\n",
    "plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Orders per Department\n",
    "p_c = data.groupby('department')['product_name'].count().sort_values()\n",
    "\n",
    "print(p_c.nlargest(10))\n",
    "\n",
    "# plot as Treemap\n",
    "import squarify\n",
    "squarify.plot(sizes=p_c.values, label=p_c.index, alpha=.8 )\n",
    "plt.axis('off')\n",
    "plt.savefig('departments_treemap.png')\n",
    "plt.show() \n",
    "\n",
    "# maybe better as a barplot\n",
    "plt.figure()\n",
    "sns.barplot(x=p_c.values, y=p_c.index)\n",
    "#plt.barh(y_pos,p_c.values)\n",
    "#plt.yticks(y_pos, p_c.index)\n",
    "#y_pos = np.arange(len(p_c.values))\n",
    "plt.xscale('log')\n",
    "plt.savefig('departments_bar.png', bbox_inches='tight')\n",
    "plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Products per Department\n",
    "n_p = data.groupby('department')['product_name'].value_counts()\n",
    "print(n_p.head(10))\n",
    "\n",
    "# Number of purchases per Category / Subcategory\n",
    "p_c_s = data.groupby('department')['aisle'].value_counts()\n",
    "print(p_c_s.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at a specific user / product transaction\n",
    "data[(data.user_id == 1) & (data.product_id == 196)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Building\n",
    "Der Code ist in die folgenden Module aufgeteilt<p>\n",
    "1. Datenaufbereitung / Preprocessing<p>\n",
    "2. Matrixerstellung<p>\n",
    "3. Recommendations - hierbei ist ein Memory-Based und ein Modell-Based Ansatz implementiert<p>\n",
    "4. Evaluation<p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datenaufbereitung\n",
    "In diesem Modul werden die Rohdaten vorbereitet bzw. gefiltert. Dies ist kein direkter Task im Kontext eines Recommender Frameworks, es ist jedoch wichtig, diesen Schritt auch zu berücksichtigen. \n",
    "Als Input in diesem Modul erfolgt \n",
    "-\tDer Datensatz, in diesem Fall sind dies Records / Transactions auf einer E-Commerce Site. (umgesetzt)\n",
    "-\tEin Threshold, dass nur Produkte (Items) berücksichtigt werden welche n mal gekauft wurden (umgesetzt)\n",
    "-\tEin Threshold, dass nur Kunden (Users) berücksichtigt werden, welche n Produkte gekauft haben (umgesetzt)\n",
    "-\tEin Threshold, dass nur Kunden berücksichtigt werden, welche mehr als einmal eingekauft haben (nicht umgesetzt)\n",
    "-\tDie Definition, wie die Daten in ein Training und ein Test-Set unterteilt werden. (umgesetzt)\n",
    "\n",
    "Als Resultat liefert das Modul einen reduzierter Datensatz, welcher die nicht relevanten Records entfernt hat und die Daten in Training und Test-Daten geteilt hat. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduktion der Transaktionen auf Basis der Produkte\n",
    "\n",
    "In der Aufgabenstellung (#Referenz einfügen) ist gefordert, dass der Produktekatalog um 80% redzuiert werden soll. In einem ersten Schritt, wird aufgelistet, welche Produkte wie oft gekauft wurden. Diese Liste wird sortiert nach der Häufigkeit der Verkaufszahlen. Anschliessend werden die Transaktionen gefiltert um nur noch Einträge zu berücksichtigen, welche den Top 20 Prozent der Produkte entsprechen.\n",
    "\n",
    "Total sind im Datensatz 49685 Produkte vorhanden. Wird diese Anzahl auf 20% reduziert, bleiben noch 9937 Produkte. Gesamthaft befinden sich im Datensatz 33'819'106 Transaktionen; gefiltert auf die Top 20% der Produkte bleiben immer noch 30'749'657 Transaktionen vorhanden.\n",
    "\n",
    "Dies führt weiter dazu, dass auch das Produkt, welches am wenigesten oft gekauft wurde, immer noch 393-mal verkauft wurde.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of products\n",
    "n_of_products = data.product_name.nunique()\n",
    "\n",
    "# 20% of the products are\n",
    "top_20 = int(n_of_products * 0.2)\n",
    "\n",
    "# select the top products\n",
    "n_of_products_bought = data.product_name.value_counts()\n",
    "prod_f = n_of_products_bought.nlargest(top_20)\n",
    "top_products = prod_f.index\n",
    "\n",
    "# filter the transactions only for the top products\n",
    "data_f = data[(data.product_name.isin(top_products))]\n",
    "\n",
    "# product most and least bought\n",
    "print('Product most bought: {0}'.format(prod_f.nlargest(1)))\n",
    "print('Product least bought (top 20%): {0}'.format(prod_f.nsmallest(1)))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduktion der Transaktionen auf Basis der User\n",
    "\n",
    "Der Datensatz wird weiter reduziert, um nur User zu erhalten welche eine relevante Anzahl an Produkte gekauft haben. Im ersten Schritt wird aufgezeigt welche User wieviele Produkte gekauft haben. Anhand dieser Zahlen wird anschliessend eine Selektion der Top n User gemacht und die nicht benötigten aus den Transaktionen entfernt.\n",
    "\n",
    "Annahme: Relevante User haben mindestens 30 verschiedene Produkte gekauft. (Muss vermutlich zu einem späteren Zeitpunkt noch angepasst werden)\n",
    "\n",
    "Durch die Filterung der Transaktionen anhand der relevaten User wird der Datensatz ein weiters mal verkleiniert, es bleiben noch 28'501'910 Transaktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of purchases per User and Product\n",
    "n_of_purch_per_user = data_f.groupby(['user_id','product_id']).size()\n",
    "# Number of Product per User\n",
    "n_of_prod_per_user = n_of_purch_per_user.groupby('user_id').size()\n",
    "\n",
    "# Users with most and least products\n",
    "print('Users with most products: {0}'.format(n_of_prod_per_user.nlargest(20)))\n",
    "print('Users with least products: {0}'.format(n_of_prod_per_user.nsmallest(20)))\n",
    "\n",
    "# Filter for Users with more than n products\n",
    "n = 30\n",
    "prod_per_user_f = n_of_prod_per_user[(n_of_prod_per_user >= n)]\n",
    "top_users = prod_per_user_f.index\n",
    "\n",
    "# Select Transactions from Users with morn than n products bought\n",
    "data_f = data_f[(data_f.user_id.isin(top_users))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstellen einer Liste aller Produkte mit ProduktID und Produktname, welche später bei der Ausgabe wiederverwendet wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products = data_f.loc[ : , ['product_id', 'product_name'] ].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unterteilung der Daten in Test- und Train-Set\n",
    "\n",
    "1. Ansatz: Zufällig 10% aller User1. Ansatz: Die Transaktionen von 10% der User gehen ins Testset, der Rest ist Trainingset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Umsetzung 1. Ansatz\n",
    "\n",
    "# Create list of all User_IDs\n",
    "user_ids = data_f.user_id.unique()\n",
    "# Select random 10% of the Users\n",
    "random_users = random.choices(user_ids, k=round(len(user_ids)*0.1))\n",
    "\n",
    "#Train Set\n",
    "data_train = data_f[~(data_f.user_id.isin(random_users))]\n",
    "\n",
    "# Test Set\n",
    "data_test = data_f[(data_f.user_id.isin(random_users))]\n",
    "\n",
    "print('Train Set is saved as data_train, Test Set is saved as data_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrixerstellung\n",
    "\n",
    "In diesem Modul wird aus dem reduzierten Datensatz eine User-Item Matrix generiert. Hierbei besteht die Möglichkeit die Ratings nach unterschiedlichen Berechnungsmethoden zu definieren. In einem ersten Ansatz wird mit einem unary / binary Rating gearbeitet. Das Rating sagt also lediglich aus, ob ein User ein Item gekauft hat (1) oder nicht (0). Nach Möglichkeiten können zu einem späteren Zeitpunkt noch weitere Berechnungsmethoden eingebaut werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrixerstellung\n",
    "\n",
    "func = lambda x: 1\n",
    "\n",
    "# Matrix for Testset\n",
    "pivot_data = data_test[['user_id','product_id']]\n",
    "matrix_test = pivot_data.pivot_table(index='user_id',columns='product_id',values='product_id',aggfunc=func,fill_value=0)\n",
    "matrix_test.reset_index(inplace=True)\n",
    "\n",
    "# Matrix for Trainset\n",
    "pivot_data = data_train[['user_id','product_id']]\n",
    "matrix_train = pivot_data.pivot_table(index='user_id',columns='product_id',values='product_id',aggfunc=func,fill_value=0)\n",
    "matrix_train.reset_index(inplace=True)\n",
    "\n",
    "matrix_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the User ID as index\n",
    "matrix_test = matrix_test.set_index('user_id')\n",
    "#matrix_train = matrix_train.set_index('user_id')\n",
    "\n",
    "# Transform matrix from pandas into numpy\n",
    "#matrix_train_np = matrix_train.to_numpy()\n",
    "matrix_test_np = matrix_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recommender\n",
    "In diesem Modul werden die eigentlichen Recommendations berechnet. Wir planen hier zwei Modelle zu implementieren:\n",
    "-\tMemory-Based: User-based collaborative filtering. Hierbei wird in der User-Item Matrix nach Ähnlichen Usern gesucht und Items vorgeschlagen, welche bei diesen Usern am beliebtesten sind. Da wir mit einem binary Rating arbeiten, muss bei der Berechnung der Predictions ein pseudo-rating berechnet werden, anhand der Ähnlichkeit zweier User zueinander. Weiter muss dem Modul mittels Parameter ein Wert mitgegeben werden, der die k-nearest Neighbours definiert, also welche User noch als ähnlich betrachtet werden.\n",
    "-\tModell-Based: Item-Item oder User-User collaborative filtering. Die User-Item Matrix wird mittels Matrix-Faktorisierung (Single Value Decomposition) in eine User-Factor und eine Factor-Item Matrix dekomposiert. Die Funktionsweisen und Details dazu müssen jedoch noch geklärt werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory-Based Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# User-User Collaborative Filtering\n",
    "# Es werden andere Kunden (Users) gesucht, welche in der Vergangenheit ein ähnliches Kaufverhalten aufweisen. Die Ratings dieser Kunden werden verwendet um vorherzusagen, wie gut ein Produkt (Item) dem Kunden gefallen könnte.\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "\n",
    "# 1. Erstellen der User-User Matrix: Berechnung der Ähnlichkeit aller User zueinander\n",
    "\n",
    "data_sparse = sparse.csr_matrix(matrix_test)\n",
    "similarities = cosine_similarity(data_sparse)\n",
    "sim = pd.DataFrame(data=similarities, index=matrix_test.index, columns=matrix_test.index)\n",
    "\n",
    "# 2. Berechnung der Nachbarschaft (Neighbourhood) auf Basis der Ähnlichkeit der User. Der Radius kann definiert werden; aktuell K=10\n",
    "k = 10\n",
    "user_neighbours = pd.DataFrame(index=matrix_test.index, columns=range(1,k+1))\n",
    "for i in range(0, len(user_neighbours.index)):\n",
    "    user_neighbours.iloc[i:i+1,0:k] = sim.iloc[i].sort_values(ascending=False)[1:k+1].index\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Berechnung der Vorhersage (Predicition) eines Produkts\n",
    "\n",
    "def predict(u,i):\n",
    "    # getting neighbourhood for the user\n",
    "    neighbourhood = user_neighbours[user_neighbours.index==u].transpose().to_numpy()\n",
    "    numerator = float(0)\n",
    "    denumerator = float(0)\n",
    "    prediction = float(0)\n",
    "\n",
    "    # Dies ist die Formel, welche von Ekstrand, Riedl & Konstan (2010, 91) im Punkt 2.6 erwähnt wird. Jedoch wurde das Durchschnittsrating entfernt, da bei einem unary Rating der Durchschnitt immer 1 ist und somit ein Rating Bias nicht möglich.\n",
    "    for neighbour in neighbourhood:\n",
    "        numerator = numerator + float(sim.loc[u,neighbour]) * int(matrix_test.loc[neighbour,i])\n",
    "        denumerator = denumerator + float(sim.loc[u,neighbour])\n",
    "    prediction = float(numerator / denumerator)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "p = predict(7,8193)\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions für alle Items in der Neighbourhood des Users berechnen, welcher dieser selbst nicht gekauft hat\n",
    "\n",
    "def products_to_recommend(u):\n",
    "    u_prod = None\n",
    "    n_prod = None\n",
    "    neighbourhood = None\n",
    "    n_prod_sum = list()\n",
    "    n_prod_check = None\n",
    "    # Alle Produkte welche der User selbst gekauft hat\n",
    "    u_prod = matrix_test.loc[u,:]\n",
    "    u_prod = u_prod[u_prod.values==1].index.tolist()\n",
    "\n",
    "    # Alle Produkte welche die Neighbours des Users gekauft haben\n",
    "    neighbourhood = user_neighbours[user_neighbours.index==u].transpose().to_numpy()\n",
    "    for neighbour in neighbourhood:\n",
    "        n_prod = matrix_test.loc[neighbour[0],:]\n",
    "        n_prod = n_prod[n_prod.values==1].index.tolist()\n",
    "        n_prod_sum = n_prod_sum + n_prod\n",
    "        n_prod_sum = list(set(n_prod_sum))\n",
    "    # Entfernen der Produkte welche der User bereits hat\n",
    "    n_prod_check = list(set(n_prod_sum) - set(u_prod))\n",
    "    return n_prod_check\n",
    "\n",
    "products_to_recommend(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Berechnen der Recommendations für einen User\n",
    "\n",
    "def get_recommendations(u,t):\n",
    "    products = None\n",
    "    recommendation = pd.DataFrame(columns=['product_name','product_id','rating'])\n",
    "    r = pd.DataFrame(columns=['item','rating'])\n",
    "    \n",
    "    # Berechenen der Produkte\n",
    "    products = products_to_recommend(u)\n",
    "\n",
    "    # Berechnen der Ratings der Produkte und sortieren\n",
    "    for item in products:\n",
    "        new_row = {'item':item,'rating':predict(u,item)}\n",
    "        r = r.append(new_row, ignore_index=True)\n",
    "    \n",
    "    r = r.sort_values(by=['rating'], ascending=False).head(t)\n",
    "    r.reset_index(inplace=True)\n",
    "    \n",
    "    # Output der Produktenamen, Produkt ID und Rating\n",
    "    i = 0\n",
    "    while (i < len(r)):\n",
    "        new_row = {'product_name':all_products[all_products.product_id==r.item[i]].product_name.values[0],'product_id':r.item[i],'rating':r.rating[i]}\n",
    "        recommendation = recommendation.append(new_row, ignore_index=True)\n",
    "        i = i + 1\n",
    "    return recommendation\n",
    "\n",
    "# Input ist die user_id sowie die Anzahl Recommendations welche ausgegeben werden sollen\n",
    "get_recommendations(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Gewichtung für jeden User\n",
    "m = np.sqrt(np.square(matrix_test).sum(axis='index'))\n",
    "# Anwendung der Gewichtung auf die jeweiligen Produkt-Ratings\n",
    "matrix_test_m = matrix_test.divide(m, axis='columns')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "Bei der Evaluation wird das Test-Set in zwei Teile aufgeteilt (der Split geschieht im Datenverarbeitungsmodul), das Test- und das Trainingset. Hierbei befinden sich 90% der User im Trainingset. Für die Daten im Testset ist zudem zu beachten, dass es sich um User handelt welche tendenziell eine grosse Anzahl an Ratings aufweisen.  Dabei wird der Recommender mit dem Training-Set angelernt und mit dem Test-Set auf seine Leistung getestet. Bei den Usern im Testset werden einzelne Ratings entfernt (maskiert), welche dann mittels des Recommenders möglichst wieder errechnet werden. Als Beispiel:\n",
    "Bei User A werden von total 40 bewerteten Items, bei 10 Items die Ratings entfernt. Anschliessend werden die Top 10 Recommendations für User A berechnet. Je grösser die Übereinstimmung der berechneten Recommendations, mit den zuvor gelöschten Items von User A ist, desto besser funktioniert der Recommender für User A. Dieser Schritt wird für sämtliche User aus dem Testset wiederholt. Der Durchschnitt aller berechneten Recommendations gegenüber ihren tatsächlichen Ratings, ergibt die Genauigkeit der Vorhersage für den verwendeten Recommender. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alles was hier noch kommt ist alt und wird in der finalen Version gelöscht. Ich lasse das aber mal noch stehen, da allenfalls mal was als noch verwendet werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyhton Packages\n",
    "\n",
    "# http://muricoca.github.io/crab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing and resource purposes, the set of transactions is reduced\n",
    "data_f = data_f.head(10000)\n",
    "\n",
    "# create a matrix with user, product and occurence as value (= number of times purchased)\n",
    "''' depricated\n",
    "matrix = pd.crosstab(data.user_id, data.product_id) \n",
    "# if we want precentage add parameter normalize=True\n",
    "\n",
    "# this could alternatively be done by groupby\n",
    "matrix_g = data.groupby(['user_id', 'product_id'])['product_id'].count().unstack().fillna(0)\n",
    "'''\n",
    "# with pivot_table - already normalized the ratings\n",
    "# this needs to be improved, maybe with sigmoid function?\n",
    "func = lambda x: x.count() / (1+x.count())\n",
    "pivot_data = data_f[['user_id','product_id']]\n",
    "matrix_p = pivot_data.pivot_table(index='user_id',columns='product_id',values='product_id',aggfunc=func,fill_value=0)\n",
    "\n",
    "matrix_p.reset_index(inplace=True)\n",
    "\n",
    "matrix_p.head(1)\n",
    "# the matrix has a lot of empty values (is very sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' depricated\n",
    "# converting the matrix back to a table for further processing\n",
    "v_table = matrix.stack().reset_index()\n",
    "v_table.columns = ['user_id','product_id','amount']\n",
    "\n",
    "# remove rows with amount = 0\n",
    "v_table = v_table[(v_table.amount != 0)]\n",
    "\n",
    "print(v_table.head())\n",
    "\n",
    "x = data.groupby(['user_id','product_id']).size()\n",
    "\n",
    "print(x.head(20))\n",
    "\n",
    "# Amount of times a product has been bought needs to be turned into a rating with range 0-1\n",
    "# Not sure if sigmoid function or percentage is better, lets try both\n",
    "v_table['rating_sig'] = v_table.amount / (1+v_table.amount)\n",
    "v_table['rating_per'] = v_table.amount / v_table.amount.sum()\n",
    "v_table.head()\n",
    "\n",
    "# Renaming the column 'amount' to rating, since in the context of a recommender it's handeled as a rating\n",
    "#v_table = v_table.rename(columns={'amount': 'rating'})\n",
    "\n",
    "#Creating a dataframe with the number of ratings and the average rating for each product\n",
    "\n",
    "# Getting the average rating of all products (Maybe this 'amount' value has to be converted to a fixed scale before?)\n",
    "ratings = pd.DataFrame(v_table.groupby('product_id')['rating_sig'].mean())\n",
    "\n",
    "# Add the number of times a Product has been bought/rated to the df\n",
    "ratings['number_of_ratings'] = v_table.groupby('product_id')['amount'].count()\n",
    "\n",
    "ratings.head()\n",
    "\n",
    "# Lets look at the distribution of the ratings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "ratings['number_of_ratings'].hist(bins=50)\n",
    "\n",
    "ratings.sort_values('number_of_ratings', ascending=False).head(10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geting a random product\n",
    "\n",
    "random_product = data_f.sample(1)\n",
    "\n",
    "print(random_product.product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a simple version of a collaborative filtering - memory based - recommender. It looks at similarities between a product and every other product in the catalog (matrix)\n",
    "# this model requires no information about the product (or user). The more transactions there are the \"better\" the recommender becomes.\n",
    "# a content-based approach is most likely not possible since there isn't much information about the product itself and no information about the user whatsoever\n",
    "\n",
    "# Calculating the correlation between a product and the catalog\n",
    "\n",
    "def recommend (product):\n",
    "    r = matrix_p.corrwith(matrix_p[product])\n",
    "    res = r.sort_values(ascending=False)[1:6] #the first product is always itself and therefore needs to be filtered\n",
    "    print(res) \n",
    "    \n",
    "    # print the results\n",
    "    prod = data_f[(data.product_id == product)].product_name.unique()\n",
    "    prod_r = data_f[(data.product_id.isin(res.index.tolist()))].product_name.unique()\n",
    "    print('For {0}, the recommended products are {1}f'.format(prod,prod_r))\n",
    "\n",
    "recommend(random_product.product_id.values[0])\n",
    "\n",
    "# with an user-user recommender: maybe filter out the products the user has already bought?\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a memory based recommender doesnt perform well if the information is sparse - https://youtu.be/v_mONWiFv0k\n",
    "# calculate the sparcity: # of ratings / total # elements\n",
    "# in theory the recommender should work better with items which have been bought by many different users. Since the matrix is built with transaction data, each product is at least bought by one person\n",
    "\n",
    "matrix_p.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bitcc3b5880bfc84dc5ad92f116411c6e95"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}